{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "#Author: Fatemeh Salehi\n",
    "Email: fatemeh.salehihafshejni@fau.de\n",
    "\n",
    "Model Training and Evaluation Notebook\n",
    "Paper: Machine Learning Prediction of Treatment Response to Biological Disease-Modifying Antirheumatic Drugs in Rheumatoid Arthritis\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import csv\n",
    "import datetime\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.calibration import calibration_curve, CalibrationDisplay\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from utils import *\n",
    "import torch\n",
    "import wget\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "# manual nested cross-validation for random forest on a classification dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "pd.set_option ('display.max_columns' , None)\n",
    "pd.set_option ('display.max_columns' , 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('modified_dataset.csv')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class distribution\n",
    "dataset= df.copy()\n",
    "print(dataset.groupby('effectiveness').size())\n",
    "print(dataset.groupby('sustained').size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual nested cross-validation for random forest on a classification dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "#matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind, chi2_contingency\n",
    "\n",
    "dataset = pd.DataFrame(df)\n",
    "\n",
    "# Create groups based on response\n",
    "responders_6_months = dataset[dataset['effectiveness'] == 1]\n",
    "non_responders_6_months = dataset[dataset['effectiveness'] == 0]\n",
    "\n",
    "# List of continuous and categorical features\n",
    "continuous_features = ['Age_years']\n",
    "categorical_features = ['Gender', 'Osteoarthritis', 'Hypertension', 'Diabetes', 'bDMARD']\n",
    "\n",
    "# Function to calculate p-values for continuous features\n",
    "def calculate_p_values_continuous(feature):\n",
    "    stat, p = ttest_ind(responders_6_months[feature], non_responders_6_months[feature], nan_policy='omit')\n",
    "    return p\n",
    "\n",
    "# Function to calculate p-values for categorical features\n",
    "def calculate_p_values_categorical(feature):\n",
    "    table = pd.crosstab(dataset['effectiveness'], dataset[feature])\n",
    "    stat, p, dof, expected = chi2_contingency(table)\n",
    "    return p\n",
    "\n",
    "# Calculate p-values\n",
    "p_values_continuous = {feature: calculate_p_values_continuous(feature) for feature in continuous_features}\n",
    "p_values_categorical = {feature: calculate_p_values_categorical(feature) for feature in categorical_features}\n",
    "\n",
    "# Display p-values\n",
    "print(\"P-values for Continuous Features:\")\n",
    "for feature, p in p_values_continuous.items():\n",
    "    print(f\"{feature}: {p}\")\n",
    "\n",
    "print(\"\\nP-values for Categorical Features:\")\n",
    "for feature, p in p_values_categorical.items():\n",
    "    print(f\"{feature}: {p}\")\n",
    "\n",
    "# Include bDMARD type and ethnicity in the table\n",
    "table_columns = continuous_features + categorical_features\n",
    "table_data = dataset[table_columns]\n",
    "\n",
    "# Save the table to a CSV file (or any other format as required)\n",
    "#table_data.to_csv('clinical_features_with_p_values.csv', index=False)\n",
    "\n",
    "#print(\"Table has been saved to clinical_features_with_p_values.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifier=pd.DataFrame(columns=['Classifier','Accuracy','AUC-ROC','F1 score', 'MCC'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADAboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "\n",
    "# Ignore all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Define your target columns and prepare the dataset\n",
    "target = ['effectiveness', 'remission', 'sustained']\n",
    "X = dataset.loc[:, ~dataset.columns.isin(target)].drop(['tptID', 'index', 'Uveitis', 'bDMARD_intake_duration_days', 'DateDiff_NM'], axis=1)\n",
    "y_effect = dataset.loc[:, 'effectiveness']\n",
    "y = y_effect.to_numpy()\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Define the random forest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "acc_results_ada = []\n",
    "precision_results_ada = []\n",
    "F1_results_ada = []\n",
    "recall_results_ada = []\n",
    "mcc_results_ada = []\n",
    "aucroc_results_ada = []\n",
    "conf_matrix_results = []\n",
    "aucs_ada = []\n",
    "y_prob_all_ADA = []\n",
    "selected_features_list = []\n",
    "best_params_list = []\n",
    "shap_values_list = []\n",
    "SHAP_values_per_fold = []\n",
    "cross_val_scores_list = []\n",
    "ada_kfold_probability = []\n",
    "kfold_true_label_ada = []\n",
    "y_prob = []\n",
    "\n",
    "# Define the AdaBoost classifier with decision tree as base estimator\n",
    "dtc = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Plotting setup\n",
    "tprs = []\n",
    "mean_fpr_ada = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "fold = 0\n",
    "\n",
    "for train_ix, test_ix in cv_outer.split(X, y):\n",
    "    # Split data\n",
    "    X_train, X_test = X[train_ix], X[test_ix]\n",
    "    y_train, y_test = y[train_ix], y[test_ix]\n",
    "    \n",
    "    # Configure the cross-validation procedure\n",
    "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    fold += 1\n",
    "    \n",
    "    model_rfc = rfc.fit(X_train, y_train)\n",
    "    feature_importances = rfc.feature_importances_\n",
    "    selected_features = np.where(feature_importances > 0.01)[0]  # set a threshold for feature selection\n",
    "    selected_features_list.append(selected_features)\n",
    "    \n",
    "    # Define the model with default hyperparameters\n",
    "    model = AdaBoostClassifier()\n",
    "    \n",
    "    # Define the grid of values to search\n",
    "    space = {\n",
    "        'n_estimators': [1, 10, 50],\n",
    "        'learning_rate': [1e-15, 1e-14, 1e-13, 1e-12, 1e-11, 1e-10, 1e-9, 1e-8, 1e-7]\n",
    "    }\n",
    "    \n",
    "    # Subset the features in the training and test sets\n",
    "    X_train_fs = X_train[:, selected_features]\n",
    "    X_test_fs = X_test[:, selected_features]\n",
    "    \n",
    "    # Define search\n",
    "    search = GridSearchCV(model, space, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "    scores = cross_val_score(search, X_train_fs, y_train, cv=cv_inner)\n",
    "    cross_val_scores_list.append(scores.mean())\n",
    "    \n",
    "    result = search.fit(X_train_fs, y_train)\n",
    "    best_params = result.best_params_\n",
    "    best_params_list.append(best_params)\n",
    "    \n",
    "    # Get the best performing model fit on the whole training set\n",
    "    best_model = result.best_estimator_\n",
    "    \n",
    "    # Evaluate model on the hold out dataset\n",
    "    def predict_proba(model, X):\n",
    "        \"\"\"Min-max scale output of `decision_function` to [0,1].\"\"\"\n",
    "        df = model.decision_function(X)\n",
    "        calibrated_df = (df - df.min()) / (df.max() - df.min())\n",
    "        proba_pos_class = np.clip(calibrated_df, 0, 1)\n",
    "        proba_neg_class = 1 - proba_pos_class\n",
    "        proba = np.c_[proba_neg_class, proba_pos_class]\n",
    "        return proba\n",
    "    \n",
    "    probas_ADA = predict_proba(best_model, X_test_fs)[:, 1]\n",
    "    y_pred_ada = best_model.predict_proba(X_test_fs)[:, 1]\n",
    "    \n",
    "    # Compute and plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_ada)\n",
    "    tprs.append(np.interp(mean_fpr_ada, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs_ada.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1.5, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (fold, roc_auc))\n",
    "    \n",
    "    yhat = best_model.predict(X_test_fs)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    acc_ada = accuracy_score(y_test, yhat)\n",
    "    precision_ada = precision_score(y_test, yhat)\n",
    "    recall_ada = recall_score(y_test, yhat)\n",
    "    f1_ada = f1_score(y_test, yhat)\n",
    "    mcc_ada = matthews_corrcoef(y_test, yhat)\n",
    "    \n",
    "    auc_ADA = auc(fpr, tpr)\n",
    "    \n",
    "    # Store the result\n",
    "    acc_results_ada.append(acc_ada)\n",
    "    precision_results_ada.append(precision_ada)\n",
    "    F1_results_ada.append(f1_ada)\n",
    "    recall_results_ada.append(recall_ada)\n",
    "    mcc_results_ada.append(mcc_ada)\n",
    "    aucroc_results_ada.append(auc_ADA)\n",
    "    ada_kfold_probability.append(probas_ADA)\n",
    "    kfold_true_label_ada.append(y_test)\n",
    "    y_prob_all_ADA.append(yhat)\n",
    "    \n",
    "    # Report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc_ada, result.best_score_, result.best_params_))\n",
    "\n",
    "# Summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(acc_results_ada), np.std(acc_results_ada)))\n",
    "print('Precision: %.3f (%.3f)' % (np.mean(precision_results_ada), np.std(precision_results_ada)))\n",
    "print('F1_Score: %.3f (%.3f)' % (np.mean(F1_results_ada), np.std(F1_results_ada)))\n",
    "print('Recall: %.3f (%.3f)' % (np.mean(recall_results_ada), np.std(recall_results_ada)))\n",
    "print('MCC: %.3f (%.3f)' % (np.mean(mcc_results_ada), np.std(mcc_results_ada)))\n",
    "print('AUC_ROC: %.3f (%.3f)' % (np.mean(aucs_ada), np.std(aucs_ada)))\n",
    "\n",
    "# Plot the mean ROC curve across all folds\n",
    "mean_tpr_ada = np.mean(tprs, axis=0)\n",
    "mean_auc_ada = auc(mean_fpr_ada, mean_tpr_ada)\n",
    "std_auc_ada = np.std(aucs_ada)\n",
    "plt.plot(mean_fpr_ada, mean_tpr_ada, color='b', label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc_ada, std_auc_ada), lw=2, alpha=.8)\n",
    "\n",
    "# Plot the chance line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "\n",
    "# Finalize the plot\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Nested Cross-Validation ROC Curve for AdaBoost Classifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_ada', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ada = pd.DataFrame(data=[['AdaBoost', '%.3f (%.3f)' % (mean(acc_results_ada), std(acc_results_ada)), \n",
    "                               '%.3f (%.3f)' % (mean(aucroc_results_ada), std(aucroc_results_ada)), \n",
    "                               '%.3f (%.3f)' % (mean(F1_results_ada), std(F1_results_ada)), \n",
    "                               '%.3f (%.3f)' % (mean(mcc_results_ada), std(mcc_results_ada))]], \n",
    "                        columns=['Classifier', 'Accuracy', 'AUC-ROC', 'F1 score', 'MCC'])\n",
    "\n",
    "data_classifier = pd.concat([data_classifier, data_ada], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from math import sqrt\n",
    "def calculate_mcc_from_confusion_matrix(y_true, y_pred):\n",
    "    # Generate the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate MCC using the formula\n",
    "    denominator = sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    if denominator == 0:\n",
    "        return 0  # Return 0 if the denominator is 0 to avoid division by zero\n",
    "    mcc = (tp * tn - fp * fn) / denominator\n",
    "    return mcc\n",
    "mcc_ada = calculate_mcc_from_confusion_matrix(y_test, yhat)\n",
    "mcc_ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_table_AUCROC = result_table_AUCROC.append({'classifiers':'AdaBoost',\n",
    "                                        'fpr':mean_fpr_ada, \n",
    "                                        'tpr':mean_tpr_ada, \n",
    "                                        'auc':mean_auc_ada}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Define your target columns and prepare the dataset\n",
    "target = ['effectiveness', 'remission', 'sustained']\n",
    "X = dataset.loc[:, ~dataset.columns.isin(target)].drop(['tptID', 'index', 'Uveitis', 'bDMARD_intake_duration_days', 'DateDiff_NM'], axis=1)\n",
    "y_effect = dataset.loc[:, 'effectiveness']\n",
    "y = y_effect.to_numpy()\n",
    "X = X.to_numpy()\n",
    "# configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# enumerate splits\n",
    "acc_results_s = list()\n",
    "precision_results_s = list()\n",
    "F1_results_s = list()\n",
    "recall_results_s = list()\n",
    "aucroc_results_s = list()\n",
    "mcc_results_s = list()\n",
    "aucs_s = list()\n",
    "\n",
    "# store probability predictions and true labels here\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "y_prob_all_ADA = list()\n",
    "\n",
    "selected_features_list = list()\n",
    "best_params_list = list()\n",
    "shap_values_list = list()\n",
    "SHAP_values_per_fold = []\n",
    "cross_val_scores_list = list()\n",
    "\n",
    "# CALIBRATION\n",
    "svm_kfold_probability = list()\n",
    "kfold_true_label_svm = list()\n",
    "y_prob = list()\n",
    "\n",
    "fold = 0\n",
    "\n",
    "mean_fpr_s = np.linspace(0, 1, 100)\n",
    "\n",
    "for train_ix, test_ix in cv_outer.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X[train_ix], X[test_ix]\n",
    "    y_train, y_test = y[train_ix], y[test_ix]\n",
    "    fold += 1\n",
    "    \n",
    "    # configure the cross-validation procedure\n",
    "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    \n",
    "    model_rfc = rfc.fit(X_train, y_train)\n",
    "    feature_importances = rfc.feature_importances_\n",
    "    selected_features = np.where(feature_importances > 0.01)[0] # set a threshold for feature selection\n",
    "    selected_features_list.append(selected_features)\n",
    "    \n",
    "    # subset the features in the training and test sets\n",
    "    X_train_fs = X_train[:, selected_features]\n",
    "    X_test_fs = X_test[:, selected_features]\n",
    "    \n",
    "    # define the model with default hyperparameters\n",
    "    param_grid = {'C': [1e-10, 1e-5, 1e-3, 1e-2, 0.1, 1, 1e1, 1e2, 1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "                  'gamma': [1e-6, 1e-5, 1e-4, 0.001, 0.005, 0.01, 0.1, 1, 10, 100]}\n",
    "    \n",
    "    # define search\n",
    "    search_s = GridSearchCV(SVC(kernel='rbf', class_weight='balanced', probability=True), param_grid, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "    \n",
    "    # execute the grid search\n",
    "    result_s = search_s.fit(X_train_fs, y_train)\n",
    "    best_model_s = result_s.best_estimator_\n",
    "    \n",
    "    # evaluate model on the hold out dataset\n",
    "    probas_svc = best_model_s.predict_proba(X_test_fs)\n",
    "    y_pred_s = best_model_s.predict_proba(X_test_fs)[:, 1]\n",
    "    \n",
    "    # Compute and plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_s)\n",
    "    tprs.append(np.interp(mean_fpr_s, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs_s.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1.5, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (fold, roc_auc))\n",
    "    \n",
    "    yhat_s = best_model_s.predict(X_test_fs)\n",
    "    \n",
    "    # evaluate the model\n",
    "    acc_s = accuracy_score(y_test, yhat_s)\n",
    "    precision_s = precision_score(y_test, yhat_s)\n",
    "    recall_s = recall_score(y_test, yhat_s)\n",
    "    f1_s = f1_score(y_test, yhat_s)\n",
    "    mcc_s = matthews_corrcoef(y_test, yhat_s)\n",
    "    auc_s = metrics.roc_auc_score(y_test, probas_svc[:, 1])\n",
    "    \n",
    "    svm_kfold_probability.append(probas_svc[:, 1])\n",
    "    kfold_true_label_svm.append(y_test)\n",
    "    \n",
    "    # store the result\n",
    "    acc_results_s.append(acc_s)\n",
    "    precision_results_s.append(precision_s)\n",
    "    F1_results_s.append(f1_s)\n",
    "    recall_results_s.append(recall_s)\n",
    "    aucroc_results_s.append(auc_s)\n",
    "    mcc_results_s.append(mcc_s)\n",
    "    \n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc_s, result_s.best_score_, result_s.best_params_))\n",
    "\n",
    "# Plot the mean ROC curve across all folds\n",
    "mean_tpr_s = np.mean(tprs, axis=0)\n",
    "mean_auc_s = auc(mean_fpr_s, mean_tpr_s)\n",
    "std_auc_s = np.std(aucs_s)\n",
    "\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(acc_results_s), std(acc_results_s)))\n",
    "print('Precision: %.3f (%.3f)' % (mean(precision_results_s), std(precision_results_s)))\n",
    "print('F1_Score: %.3f (%.3f)' % (mean(F1_results_s), std(F1_results_s)))\n",
    "print('Recall: %.3f (%.3f)' % (mean(recall_results_s), std(recall_results_s)))\n",
    "print('AUC_ROC: %.3f (%.3f)' % (mean_auc_s, std_auc_s))\n",
    "print('MCC: %.3f (%.3f)' % (mean(mcc_results_s), std(mcc_results_s)))\n",
    "\n",
    "# Results\n",
    "all_scores_svc = {\n",
    "    \"Accuracy\": acc_results_s,\n",
    "    'AUC-ROC': aucroc_results_s,\n",
    "    'F1-Score': F1_results_s,\n",
    "    'Recall': recall_results_s,\n",
    "    'Precision': precision_results_s,\n",
    "    'MCC': mcc_results_s,\n",
    "    'Classifier': 'SVM'\n",
    "}\n",
    "\n",
    "all_scores_svc = pd.DataFrame(all_scores_svc)\n",
    "\n",
    "plt.plot(mean_fpr_s, mean_tpr_s, color='b', label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc_s, std_auc_s), lw=2, alpha=.8)\n",
    "\n",
    "# Plot the chance line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "\n",
    "# Finalize the plot\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Nested Cross-Validation ROC Curve for SVM Classifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_svm', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s=pd.DataFrame(data= [['SVM','%.3f (%.3f)' %(mean(acc_results_s), std(acc_results_s)),'%.3f (%.3f)' % (mean(aucroc_results_s), std(aucroc_results_s)),'  %.3f (%.3f)' % (mean(F1_results_s), std(F1_results_s))]],columns=['Classifier','Accuracy','AUC-ROC','F1 score'])\n",
    "data_classifier=data_classifier.append(data_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = pd.DataFrame(data=[['SVM',\n",
    "                             '%.3f (%.3f)' % (mean(acc_results_s), std(acc_results_s)),\n",
    "                             '%.3f (%.3f)' % (mean(aucroc_results_s), std(aucroc_results_s)),\n",
    "                             '%.3f (%.3f)' % (mean(F1_results_s), std(F1_results_s)),\n",
    "                             '%.3f (%.3f)' % (mean(mcc_results_s), std(mcc_results_s))]],\n",
    "                      columns=['Classifier', 'Accuracy', 'AUC-ROC', 'F1 score', 'MCC'])\n",
    "\n",
    "data_classifier = pd.concat([data_classifier, data_s], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
    "\n",
    "# Configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize lists to store results\n",
    "acc_results_r = []\n",
    "precision_results_r = []\n",
    "F1_results_r = []\n",
    "recall_results_r = []\n",
    "aucroc_results_r = []\n",
    "mcc_results_r = []\n",
    "aucs_r = []\n",
    "tprs = []\n",
    "\n",
    "# Store probability predictions and true labels here\n",
    "r_kfold_probability = []\n",
    "kfold_true_label_r = []\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "fold = 0\n",
    "\n",
    "mean_fpr_r = np.linspace(0, 1, 100)\n",
    "\n",
    "# Optimal threshold\n",
    "optimal_threshold = 0.5  # You might want to optimize this further\n",
    "\n",
    "for train_ix, test_ix in cv_outer.split(X):\n",
    "    # Split data\n",
    "    X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "    y_train, y_test = y[train_ix], y[test_ix]\n",
    "    fold += 1\n",
    "    \n",
    "    # Configure the cross-validation procedure\n",
    "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    \n",
    "    # Feature selection\n",
    "    model_rfc = rfc.fit(X_train, y_train)\n",
    "    feature_importances = model_rfc.feature_importances_\n",
    "    selected_features = np.where(feature_importances > 0.01)[0]  # Set a threshold for feature selection\n",
    "    selected_features_list.append(selected_features)\n",
    "    \n",
    "    # Subset the features in the training and test sets\n",
    "    X_train_fs = X_train[:, selected_features]\n",
    "    X_test_fs = X_test[:, selected_features]\n",
    "    \n",
    "    # Define the model with default hyperparameters\n",
    "    model = RandomForestClassifier(random_state=22)\n",
    "    \n",
    "    # Define the grid of values to search\n",
    "    param_grid = { \n",
    "        'n_estimators': [1, 10, 100],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [1, 2, 3, 4],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "    \n",
    "    # Define search\n",
    "    search = GridSearchCV(model, param_grid, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "    \n",
    "    # Execute the grid search\n",
    "    result = search.fit(X_train_fs, y_train)\n",
    "    best_model = result.best_estimator_\n",
    "    \n",
    "    y_pred_r = best_model.predict_proba(X_test_fs)[:, 1]\n",
    "    \n",
    "    # Compute and plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_r)\n",
    "    tprs.append(np.interp(mean_fpr_r, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs_r.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1.5, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (fold, roc_auc))\n",
    "    \n",
    "    # Evaluate model on the hold out dataset\n",
    "    probas_r = best_model.predict_proba(X_test_fs)\n",
    "    yhat_r = best_model.predict(X_test_fs)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    acc_r = accuracy_score(y_test, yhat_r)\n",
    "    precision_r = precision_score(y_test, yhat_r)\n",
    "    recall_r = recall_score(y_test, yhat_r)\n",
    "    f1_r = f1_score(y_test, yhat_r)\n",
    "    \n",
    "    # Optimize MCC with the threshold\n",
    "    yhat_r_optimized = (probas_r[:, 1] >= optimal_threshold).astype(int)\n",
    "    mcc_r = matthews_corrcoef(y_test, yhat_r_optimized)\n",
    "    \n",
    "    # Evaluate AUC\n",
    "    auc_r = metrics.roc_auc_score(y_test, probas_r[:, 1])\n",
    "    \n",
    "    r_kfold_probability.append(probas_r[:, 1])\n",
    "    kfold_true_label_r.append(y_test)\n",
    "    \n",
    "    # Store the result\n",
    "    acc_results_r.append(acc_r)\n",
    "    precision_results_r.append(precision_r)\n",
    "    F1_results_r.append(f1_r)\n",
    "    recall_results_r.append(recall_r)\n",
    "    aucroc_results_r.append(auc_r)\n",
    "    mcc_results_r.append(mcc_r)\n",
    "    \n",
    "    # Report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc_r, result.best_score_, result.best_params_))\n",
    "\n",
    "# Plot the mean ROC curve across all folds\n",
    "mean_tpr_r = np.mean(tprs, axis=0)\n",
    "mean_auc_r = auc(mean_fpr_r, mean_tpr_r)\n",
    "std_auc_r = np.std(aucs_r)\n",
    "\n",
    "# Summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(acc_results_r), std(acc_results_r)))\n",
    "print('Precision: %.3f (%.3f)' % (mean(precision_results_r), std(precision_results_r)))\n",
    "print('F1_Score: %.3f (%.3f)' % (mean(F1_results_r), std(F1_results_r)))\n",
    "print('Recall: %.3f (%.3f)' % (mean(recall_results_r), std(recall_results_r)))\n",
    "print('AUC_ROC: %.3f (%.3f)' % (mean(aucroc_results_r), std(aucroc_results_r)))\n",
    "print('MCC: %.3f (%.3f)' % (mean(mcc_results_r), std(mcc_results_r)))\n",
    "\n",
    "# Results\n",
    "all_scores_rf = {\n",
    "    \"Accuracy\": acc_results_r,\n",
    "    'AUC-ROC': aucroc_results_r,\n",
    "    'F1-Score': F1_results_r,\n",
    "    'Recall': recall_results_r,\n",
    "    'Precision': precision_results_r,\n",
    "    'MCC': mcc_results_r,\n",
    "    'Classifier': 'Random Forest'\n",
    "}\n",
    "\n",
    "all_scores_rf = pd.DataFrame(all_scores_rf)\n",
    "\n",
    "plt.plot(mean_fpr_r, mean_tpr_r, color='b', label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc_r, std_auc_r), lw=2, alpha=.8)\n",
    "\n",
    "# Plot the chance line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "\n",
    "# Finalize the plot\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Nested Cross-Validation ROC Curve for Random Forest Classifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_rforest', dpi=300)\n",
    "\n",
    "# Append new data to data_classifier\n",
    "data_rf = pd.DataFrame(data=[['Random Forest',\n",
    "                              '%.3f (%.3f)' % (mean(acc_results_r), std(acc_results_r)),\n",
    "                              '%.3f (%.3f)' % (mean(aucroc_results_r), std(aucroc_results_r)),\n",
    "                              '%.3f (%.3f)' % (mean(F1_results_r), std(F1_results_r)),\n",
    "                              '%.3f (%.3f)' % (mean(mcc_results_r), std(mcc_results_r))]],\n",
    "                       columns=['Classifier', 'Accuracy', 'AUC-ROC', 'F1 score', 'MCC'])\n",
    "\n",
    "data_classifier = pd.concat([data_classifier, data_rf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import shap\n",
    "\n",
    "# Configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize lists to store results\n",
    "acc_results_x = []\n",
    "precision_results_x = []\n",
    "F1_results_x = []\n",
    "recall_results_x = []\n",
    "aucroc_results_x = []\n",
    "mcc_results_x = []\n",
    "aucs_x = []\n",
    "tprs = []\n",
    "\n",
    "# Store probability predictions and true labels here\n",
    "x_kfold_probability = []\n",
    "kfold_true_label_x = []\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "fold = 0\n",
    "iter_shap = 0\n",
    "indices = []\n",
    "df_avg_train = []\n",
    "df_avg_test = []\n",
    "\n",
    "mean_fpr_x = np.linspace(0, 1, 100)\n",
    "\n",
    "# Optimal threshold\n",
    "optimal_threshold = 0.5  # You might want to optimize this further\n",
    "\n",
    "for train_ix, test_ix in cv_outer.split(X):\n",
    "    fold += 1\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    \n",
    "    # Configure the cross-validation procedure\n",
    "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    \n",
    "    # Feature selection\n",
    "    model_rfc = rfc.fit(X_train, y_train)\n",
    "    feature_importances = model_rfc.feature_importances_\n",
    "    selected_features = np.where(feature_importances > 0.01)[0]  # Set a threshold for feature selection\n",
    "    selected_features_list.append(selected_features)\n",
    "   \n",
    "    # Subset the features in the training and test sets\n",
    "    X_train_fs = X_train.iloc[:, selected_features]\n",
    "    X_test_fs = X_test.iloc[:, selected_features]\n",
    "    \n",
    "    # Define the model with default hyperparameters\n",
    "    model = XGBClassifier(n_jobs=-1)\n",
    "    \n",
    "    # Define a smaller grid of values to search\n",
    "    param_grid_d = {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.01, 0.1],\n",
    "        'reg_lambda': [0, 0.01, 0.1],\n",
    "        'n_estimators': [50, 100, 200]\n",
    "    }\n",
    "    param_grid = {'n_estimators': [10,20, 5,50, 100, 150], 'max_depth': [0.5,1,2, 3,4, 6], 'learning_rate': [1e-20,1e-15,1e-3,1e-2,1,0.1, 0.01, 0.001,10]}\n",
    "\n",
    "    # Define search\n",
    "    search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=50, scoring='accuracy', cv=cv_inner, refit=True, n_jobs=-1, random_state=1)\n",
    "    \n",
    "    # Execute the grid search\n",
    "    result = search.fit(X_train_fs, y_train)\n",
    "    best_model = result.best_estimator_\n",
    "    \n",
    "    y_pred_x = best_model.predict_proba(X_test_fs)[:, 1]\n",
    "    \n",
    "    # Compute and plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_x)\n",
    "    tprs.append(np.interp(mean_fpr_x, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs_x.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1.5, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (fold, roc_auc))\n",
    "    \n",
    "    # Evaluate model on the hold out dataset\n",
    "    probas_x = best_model.predict_proba(X_test_fs)\n",
    "\n",
    "    yhat_x = best_model.predict(X_test_fs)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    acc_x = accuracy_score(y_test, yhat_x)\n",
    "    precision_x = precision_score(y_test, yhat_x)\n",
    "    recall_x = recall_score(y_test, yhat_x)\n",
    "    f1_x = f1_score(y_test, yhat_x)\n",
    "    \n",
    "    # Optimize MCC with the threshold\n",
    "    yhat_x_optimized = (probas_x[:, 1] >= optimal_threshold).astype(int)\n",
    "    mcc_x = matthews_corrcoef(y_test, yhat)\n",
    "    \n",
    "    # Evaluate AUC\n",
    "    auc_x = metrics.roc_auc_score(y_test, probas_x[:, 1])\n",
    "     # Store the result\n",
    "    acc_results_x.append(acc_x)\n",
    "    precision_results_x.append(precision_x)\n",
    "    recall_results_x.append(recall_x)\n",
    "    F1_results_x.append(f1_x)\n",
    "    aucroc_results_x.append(auc_x)\n",
    "    mcc_results_x.append(mcc_x)\n",
    "    \n",
    "    # Report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc_x, result.best_score_, result.best_params_))\n",
    "     \n",
    "    ## Calibration\n",
    "    x_kfold_probability.append(probas_x[:, 1])\n",
    "    kfold_true_label_x.append(y_test)\n",
    "    \n",
    "   \n",
    "    \n",
    "   \n",
    " \n",
    "    # Defining the SHAP value classifier\n",
    "    classifier_shap = best_model\n",
    "    classifier_shap.fit(X_train, y_train)\n",
    "\n",
    "    # SHAP values for training data\n",
    "    train_tmp_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "    explainer = shap.TreeExplainer(classifier_shap, train_tmp_df, feature_perturbation='interventional', model_output='probability')\n",
    "\n",
    "    # SHAP values for test data\n",
    "    test_tmp_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "    test_explainer = shap.TreeExplainer(classifier_shap, test_tmp_df, feature_perturbation='interventional', model_output='probability')\n",
    "\n",
    "    df_shap_train = pd.DataFrame(explainer.shap_values(train_tmp_df), columns=X.columns)\n",
    "    df_shap_train[\"index\"] = train_ix\n",
    "    df_avg_train.append(df_shap_train)\n",
    "\n",
    "    test_df_shap = pd.DataFrame(test_explainer.shap_values(test_tmp_df), columns=X.columns)\n",
    "    test_df_shap[\"index\"] = test_ix\n",
    "    df_avg_test.append(test_df_shap)\n",
    "\n",
    "    iter_shap += 1\n",
    "\n",
    "# Summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(acc_results_x), std(acc_results_x)))\n",
    "print('Precision: %.3f (%.3f)' % (mean(precision_results_x), std(precision_results_x)))\n",
    "print('F1_Score: %.3f (%.3f)' % (mean(F1_results_x), std(F1_results_x)))\n",
    "print('Recall: %.3f (%.3f)' % (mean(recall_results_x), std(recall_results_x)))\n",
    "print('AUC_ROC: %.3f (%.3f)' % (mean(aucroc_results_x), std(aucroc_results_x)))\n",
    "print('MCC: %.3f (%.3f)' % (mean(mcc_results_x), std(mcc_results_x)))\n",
    "\n",
    "# Results\n",
    "all_scores_x = {\n",
    "    \"Accuracy\": acc_results_x,\n",
    "    'AUC-ROC': aucroc_results_x,\n",
    "    'F1-Score': F1_results_x,\n",
    "    'Recall': recall_results_x,\n",
    "    'Precision': precision_results_x,\n",
    "    'MCC': mcc_results_x,\n",
    "    'Classifier': 'XGBoost'\n",
    "}\n",
    "\n",
    "all_scores_x = pd.DataFrame(all_scores_x)\n",
    "\n",
    "# Plot the mean ROC curve across all folds\n",
    "mean_tpr_x = np.mean(tprs, axis=0)\n",
    "mean_auc_x = auc(mean_fpr_x, mean_tpr_x)\n",
    "std_auc_x = np.std(aucs_x)\n",
    "\n",
    "plt.plot(mean_fpr_x, mean_tpr_x, color='b', label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc_x, std_auc_x), lw=2, alpha=.8)\n",
    "\n",
    "# Plot the chance line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "\n",
    "# Finalize the plot\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Nested Cross-Validation ROC Curve for XGBoost Classifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_xgboost', dpi=300)\n",
    "\n",
    "# Append new data to data_classifier\n",
    "data_xgboost = pd.DataFrame(data=[['XGBoost',\n",
    "                                   '%.3f (%.3f)' % (mean(acc_results_x), std(acc_results_x)),\n",
    "                                   '%.3f (%.3f)' % (mean(aucroc_results_x), std(aucroc_results_x)),\n",
    "                                   '%.3f (%.3f)' % (mean(F1_results_x), std(F1_results_x)),\n",
    "                                   '%.3f (%.3f)' % (mean(mcc_results_x), std(mcc_results_x))]],\n",
    "                            columns=['Classifier', 'Accuracy', 'AUC-ROC', 'F1 score', 'MCC'])\n",
    "\n",
    "data_classifier = pd.concat([data_classifier, data_xgboost], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import shap\n",
    "\n",
    "# Load data\n",
    "# Normalize\n",
    "target = ['remission', 'del_das', 'effectiveness', 'effect_n']\n",
    "# Select all columns except 'col_nandata'\n",
    "X = dataset.loc[:, ~dataset.columns.isin(target)].drop(['tptID', 'index', 'Uveitis', 'bDMARD_intake_duration_days', 'DateDiff_NM'], axis=1)\n",
    "\n",
    "y_effect = dataset.loc[:, 'effectiveness']\n",
    "y = y_effect\n",
    "\n",
    "# Configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize lists to store results\n",
    "acc_results_x = []\n",
    "precision_results_x = []\n",
    "F1_results_x = []\n",
    "recall_results_x = []\n",
    "aucroc_results_x = []\n",
    "mcc_results_x = []\n",
    "aucs_x = []\n",
    "tprs = []\n",
    "\n",
    "# Store probability predictions and true labels here\n",
    "x_kfold_probability = []\n",
    "kfold_true_label_x = []\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "fold = 0\n",
    "iter_shap = 0\n",
    "indices = []\n",
    "df_avg_train = []\n",
    "df_avg_test = []\n",
    "\n",
    "mean_fpr_x = np.linspace(0, 1, 100)\n",
    "\n",
    "for train_ix, test_ix in cv_outer.split(X):\n",
    "    fold += 1\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    \n",
    "    # Configure the cross-validation procedure\n",
    "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    \n",
    "    # Feature selection\n",
    "    model_rfc = rfc.fit(X_train, y_train)\n",
    "    feature_importances = model_rfc.feature_importances_\n",
    "    selected_features = np.where(feature_importances > 0.01)[0]  # Set a threshold for feature selection\n",
    "    selected_features_list.append(selected_features)\n",
    "   \n",
    "    # Subset the features in the training and test sets\n",
    "    X_train_fs = X_train.iloc[:, selected_features]\n",
    "    X_test_fs = X_test.iloc[:, selected_features]\n",
    "    \n",
    "    # Define the model with default hyperparameters\n",
    "    model = XGBClassifier(n_jobs=-1)\n",
    "    \n",
    "    # Define the grid of values to search\n",
    "    param_grid_d = {\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "        'max_depth': [3, 5, 7, 9, 11],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'reg_alpha': [0, 0.001, 0.01, 0.1, 0.5],\n",
    "        'reg_lambda': [0, 0.001, 0.01, 0.1, 0.5],\n",
    "        'n_estimators': [10, 100, 200, 500]\n",
    "    }\n",
    "    # Define a smaller grid of values to search\n",
    "    param_grid = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0,0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # Define search\n",
    "    search = GridSearchCV(model, param_grid, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "    \n",
    "    # Execute the grid search\n",
    "    result = search.fit(X_train_fs, y_train)\n",
    "    best_model = result.best_estimator_\n",
    "    \n",
    "    y_pred_x = best_model.predict_proba(X_test_fs)[:, 1]\n",
    "    \n",
    "    # Compute and plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_x)\n",
    "    tprs.append(np.interp(mean_fpr_x, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs_x.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1.5, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (fold, roc_auc))\n",
    "    \n",
    "    # Evaluate model on the hold out dataset\n",
    "    probas_x = best_model.predict_proba(X_test_fs)\n",
    "\n",
    "    yhat_x = best_model.predict(X_test_fs)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    acc_x = accuracy_score(y_test, yhat_x)\n",
    "    precision_x = precision_score(y_test, yhat_x)\n",
    "    recall_x = recall_score(y_test, yhat_x)\n",
    "    f1_x = f1_score(y_test, yhat_x)\n",
    "    mcc_x = matthews_corrcoef(y_test, yhat_x)\n",
    "    \n",
    "    # Evaluate AUC\n",
    "    auc_x = metrics.roc_auc_score(y_test, probas_x[:, 1])\n",
    "    \n",
    "    ## Calibration\n",
    "    # x_kfold_probability.append(probas_x[:, 1])\n",
    "    # kfold_true_label_x.append(y_test)\n",
    "    \n",
    "    # Store the result\n",
    "    acc_results_x.append(acc_x)\n",
    "    precision_results_x.append(precision_x)\n",
    "    recall_results_x.append(recall_x)\n",
    "    F1_results_x.append(f1_x)\n",
    "    aucroc_results_x.append(auc_x)\n",
    "    mcc_results_x.append(mcc_x)\n",
    "    \n",
    "    # Report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc_x, result.best_score_, result.best_params_))\n",
    "    \n",
    "    # Defining the SHAP value classifier\n",
    "    # classifier_shap = best_model\n",
    "    # classifier_shap.fit(X_train, y_train)\n",
    "\n",
    "    # ############## SHAP VALUES ################\n",
    "    # train_tmp_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "    # explainer = shap.TreeExplainer(classifier_shap, train_tmp_df, feature_perturbation='interventional', model_output='probability')\n",
    "\n",
    "    # ### TEST DATA\n",
    "    # test_tmp_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "    # test_explainer = shap.TreeExplainer(classifier_shap, test_tmp_df, feature_perturbation='interventional', model_output='probability')\n",
    "\n",
    "    # df_shap_train = pd.DataFrame(explainer.shap_values(train_tmp_df), columns=X.columns)\n",
    "    # df_shap_train[\"index\"] = train_ix\n",
    "\n",
    "    # df_avg_train.append(df_shap_train)\n",
    "\n",
    "    # test_df_shap = pd.DataFrame(test_explainer.shap_values(test_tmp_df), columns=X.columns)\n",
    "    # test_df_shap[\"index\"] = test_ix\n",
    "    # df_avg_test.append(test_df_shap)\n",
    "\n",
    "    # iter_shap += 1\n",
    "\n",
    "# Summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(acc_results_x), std(acc_results_x)))\n",
    "print('Precision: %.3f (%.3f)' % (mean(precision_results_x), std(precision_results_x)))\n",
    "print('F1_Score: %.3f (%.3f)' % (mean(F1_results_x), std(F1_results_x)))\n",
    "print('Recall: %.3f (%.3f)' % (mean(recall_results_x), std(recall_results_x)))\n",
    "print('AUC_ROC: %.3f (%.3f)' % (mean(aucroc_results_x), std(aucroc_results_x)))\n",
    "print('MCC: %.3f (%.3f)' % (mean(mcc_results_x), std(mcc_results_x)))\n",
    "\n",
    "# Results\n",
    "all_scores_x = {\n",
    "    \"Accuracy\": acc_results_x,\n",
    "    'AUC-ROC': aucroc_results_x,\n",
    "    'F1-Score': F1_results_x,\n",
    "    'Recall': recall_results_x,\n",
    "    'Precision': precision_results_x,\n",
    "    'MCC': mcc_results_x,\n",
    "    'Classifier': 'XGBoost'\n",
    "}\n",
    "\n",
    "all_scores_x = pd.DataFrame(all_scores_x)\n",
    "\n",
    "# Plot the mean ROC curve across all folds\n",
    "mean_tpr_x = np.mean(tprs, axis=0)\n",
    "mean_auc_x = auc(mean_fpr_x, mean_tpr_x)\n",
    "std_auc_x = np.std(aucs_x)\n",
    "\n",
    "plt.plot(mean_fpr_x, mean_tpr_x, color='b', label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc_x, std_auc_x), lw=2, alpha=.8)\n",
    "\n",
    "# Plot the chance line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "\n",
    "# Finalize the plot\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Nested Cross-Validation ROC Curve for XGBoost Classifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_xgboost', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append new data to data_classifier\n",
    "data_xgboost = pd.DataFrame(data=[['XGBoost',\n",
    "                                   '%.3f (%.3f)' % (mean(acc_results_x), std(acc_results_x)),\n",
    "                                   '%.3f (%.3f)' % (mean(aucroc_results_x), std(aucroc_results_x)),\n",
    "                                   '%.3f (%.3f)' % (mean(F1_results_x), std(F1_results_x)),\n",
    "                                   '%.3f (%.3f)' % (mean(mcc_results_x), std(mcc_results_x))]],\n",
    "                            columns=['Classifier', 'Accuracy', 'AUC-ROC', 'F1 score', 'MCC'])\n",
    "\n",
    "data_classifier = pd.concat([data_classifier, data_xgboost], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['effectiveness', 'remission', 'sustained']\n",
    "X = dataset.loc[:, ~dataset.columns.isin(target)].drop(['tptID', 'index', 'Uveitis', 'bDMARD_intake_duration_days', 'DateDiff_NM'], axis=1)\n",
    "y_effect = dataset.loc[:, 'effectiveness']\n",
    "y = y_effect.to_numpy()\n",
    "X = X.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Configuration for cross-validation\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize results storage\n",
    "acc_results_k = []\n",
    "precision_results_k = []\n",
    "F1_results_k = []\n",
    "recall_results_k = []\n",
    "aucroc_results_k = []\n",
    "mcc_results_k = []\n",
    "\n",
    "# ROC Curve preparation\n",
    "tprs = []\n",
    "aucs_k = []\n",
    "mean_fpr_k = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "fold = 0\n",
    "\n",
    "# Main cross-validation loop\n",
    "for train_ix, test_ix in cv_outer.split(X):\n",
    "    fold += 1\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "    y_train, y_test = y[train_ix], y[test_ix]\n",
    "    \n",
    "    # Feature selection using RandomForestClassifier\n",
    "    rfc = RandomForestClassifier(random_state=1)\n",
    "    model_rfc = rfc.fit(X_train, y_train)\n",
    "    feature_importances = rfc.feature_importances_\n",
    "    selected_features = np.where(feature_importances > 0.01)[0]\n",
    "    \n",
    "    # Subset the features in the training and test sets\n",
    "    X_train_fs, X_test_fs = X_train[:, selected_features], X_test[:, selected_features]\n",
    "    \n",
    "    # Define and configure the KNN model\n",
    "    model = KNeighborsClassifier()\n",
    "    param_grid = {\n",
    "        'n_neighbors': [1, 3, 5, 7, 6, 8, 10, 12],\n",
    "        'leaf_size': list(range(1, 50))\n",
    "    }\n",
    "    \n",
    "    # Grid search within the inner cross-validation loop\n",
    "    search_k = GridSearchCV(model, param_grid, scoring='accuracy', cv=KFold(n_splits=3, shuffle=True, random_state=1), refit=True)\n",
    "    result = search_k.fit(X_train_fs, y_train)\n",
    "    best_model = result.best_estimator_\n",
    "    \n",
    "    # Predict probabilities and calculate ROC curve\n",
    "    y_pred_k = best_model.predict_proba(X_test_fs)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_k)\n",
    "    tprs.append(np.interp(mean_fpr_k, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs_k.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1.5, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (fold, roc_auc))\n",
    "\n",
    "    # Predictions for evaluation\n",
    "    yhat_k = best_model.predict(X_test_fs)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    acc_k = accuracy_score(y_test, yhat_k)\n",
    "    precision_k = precision_score(y_test, yhat_k)\n",
    "    recall_k = recall_score(y_test, yhat_k)\n",
    "    f1_k = f1_score(y_test, yhat_k)\n",
    "    mcc_k = matthews_corrcoef(y_test, yhat_k)\n",
    "    \n",
    "    # Store results\n",
    "    acc_results_k.append(acc_k)\n",
    "    precision_results_k.append(precision_k)\n",
    "    F1_results_k.append(f1_k)\n",
    "    recall_results_k.append(recall_k)\n",
    "    aucroc_results_k.append(roc_auc)\n",
    "    mcc_results_k.append(mcc_k)\n",
    "    \n",
    "    # Report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s, mcc=%.3f' % (acc_k, result.best_score_, result.best_params_, mcc_k))\n",
    "\n",
    "# Summarize estimated performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(acc_results_k), std(acc_results_k)))\n",
    "print('Precision: %.3f (%.3f)' % (mean(precision_results_k), std(precision_results_k)))\n",
    "print('F1_Score: %.3f (%.3f)' % (mean(F1_results_k), std(F1_results_k)))\n",
    "print('Recall: %.3f (%.3f)' % (mean(recall_results_k), std(recall_results_k)))\n",
    "print('AUC_ROC: %.3f (%.3f)' % (mean(aucroc_results_k), std(aucroc_results_k)))\n",
    "print('MCC: %.3f (%.3f)' % (mean(mcc_results_k), std(mcc_results_k)))\n",
    "\n",
    "# Plot the mean ROC curve across all folds\n",
    "mean_tpr_k = np.mean(tprs, axis=0)\n",
    "mean_auc_k = auc(mean_fpr_k, mean_tpr_k)\n",
    "std_auc_k = np.std(aucs_k)\n",
    "plt.plot(mean_fpr_k, mean_tpr_k, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc_k, std_auc_k), lw=2, alpha=.8)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Nested Cross-Validation ROC Curve for KNN Classifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_knn', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sustained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('modified_dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "target=['remission','del_das', 'effectiveness','effect_n','sustained']\n",
    "#select all columns except 'col_nandata'\n",
    "X_sustained=df.loc[:, ~dataset.columns.isin(target)].drop(['tptID', 'index','Uveitis','bDMARD_intake_duration_days','DateDiff_NM'], axis=1)\n",
    "X_s=X_sustained\n",
    "\n",
    "y_effect_s=df.loc[:,'sustained']\n",
    "y_s= y_effect_s\n",
    "\n",
    "X_s=X_s.to_numpy()\n",
    "\n",
    "y_s=y_s.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifier_s=pd.DataFrame(columns=['Classifier','Accuracy','AUC-ROC','F1 score','MCC'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADAboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ignore all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Define the random forest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "acc_results_ada = []\n",
    "precision_results_ada = []\n",
    "F1_results_ada = []\n",
    "recall_results_ada = []\n",
    "aucroc_results_ada = []\n",
    "mcc_results_ada = []\n",
    "aucs_ada = []\n",
    "\n",
    "# Store probability predictions and true labels here\n",
    "y_prob_all_ADA = []\n",
    "selected_features_list = []\n",
    "best_params_list = []\n",
    "cross_val_scores_list = []\n",
    "ada_kfold_probability = []\n",
    "kfold_true_label_ada = []\n",
    "\n",
    "# Define the AdaBoost classifier with decision tree as base estimator\n",
    "dtc = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "tprs = []\n",
    "mean_fpr_ada_s = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "fold = 0\n",
    "\n",
    "for train_ix, test_ix in cv_outer.split(X_s, y_s):\n",
    "    # Split data\n",
    "    X_train, X_test = X_s[train_ix], X_s[test_ix]\n",
    "    y_train, y_test = y_s[train_ix], y_s[test_ix]\n",
    "    \n",
    "    # Configure the cross-validation procedure\n",
    "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    fold += 1\n",
    "    \n",
    "    # Feature selection\n",
    "    model_rfc = rfc.fit(X_train, y_train)\n",
    "    feature_importances = model_rfc.feature_importances_\n",
    "    selected_features = np.where(feature_importances > 0.01)[0]  # Set a threshold for feature selection\n",
    "    selected_features_list.append(selected_features)\n",
    "    \n",
    "    # Define the model with default hyperparameters\n",
    "    model = AdaBoostClassifier()\n",
    "    \n",
    "    # Define the grid of values to search\n",
    "    space = {\n",
    "        'n_estimators': [1, 10, 50],\n",
    "        'learning_rate': [1e-15, 1e-14, 1e-13, 1e-12, 1e-11, 1e-10, 1e-9, 1e-8, 1e-7]\n",
    "    }\n",
    "\n",
    "    # Subset the features in the training and test sets\n",
    "    X_train_fs = X_train[:, selected_features]\n",
    "    X_test_fs = X_test[:, selected_features]\n",
    "    \n",
    "    # Define search\n",
    "    search = GridSearchCV(model, space, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "    scores = cross_val_score(search, X_train_fs, y_train, cv=cv_inner)\n",
    "    cross_val_scores_list.append(scores.mean())\n",
    "    \n",
    "    # Execute the grid search\n",
    "    result = search.fit(X_train_fs, y_train)\n",
    "    best_params = result.best_params_\n",
    "    best_params_list.append(best_params)\n",
    "    \n",
    "    # Get the best performing model fit on the whole training set\n",
    "    best_model = result.best_estimator_\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Min-max scale output of `decision_function` to [0, 1].\"\"\"\n",
    "        df = self.decision_function(X)\n",
    "        calibrated_df = (df - df.min()) / (df.max() - df.min())\n",
    "        proba_pos_class = np.clip(calibrated_df, 0, 1)\n",
    "        proba_neg_class = 1 - proba_pos_class\n",
    "        proba = np.c_[proba_neg_class, proba_pos_class]\n",
    "        return proba\n",
    "\n",
    "    probas_ADA = predict_proba(best_model, X_test_fs)[:, 1]\n",
    "    y_pred_ada = best_model.predict_proba(X_test_fs)[:, 1]\n",
    "    \n",
    "    # Compute and plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_ada)\n",
    "    tprs.append(np.interp(mean_fpr_ada_s, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs_ada.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1.5, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (fold, roc_auc))\n",
    "    \n",
    "    yhat = best_model.predict(X_test_fs)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    acc_ada = accuracy_score(y_test, yhat)\n",
    "    precision_ada = precision_score(y_test, yhat)\n",
    "    recall_ada = recall_score(y_test, yhat)\n",
    "    f1_ada = f1_score(y_test, yhat)\n",
    "    mcc_ada = matthews_corrcoef(y_test, yhat)\n",
    "    auc_ADA = metrics.roc_auc_score(y_test, probas_ADA)\n",
    "    \n",
    "    # Store the result\n",
    "    acc_results_ada.append(acc_ada)\n",
    "    precision_results_ada.append(precision_ada)\n",
    "    recall_results_ada.append(recall_ada)\n",
    "    F1_results_ada.append(f1_ada)\n",
    "    aucroc_results_ada.append(auc_ADA)\n",
    "    mcc_results_ada.append(mcc_ada)\n",
    "    ada_kfold_probability.append(probas_ADA)\n",
    "    kfold_true_label_ada.append(y_test)\n",
    "    y_prob_all_ADA.append(yhat)\n",
    "    \n",
    "    # Report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc_ada, result.best_score_, result.best_params_))\n",
    "\n",
    "# Summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(acc_results_ada), std(acc_results_ada)))\n",
    "print('Precision: %.3f (%.3f)' % (mean(precision_results_ada), std(precision_results_ada)))\n",
    "print('F1_Score: %.3f (%.3f)' % (mean(F1_results_ada), std(F1_results_ada)))\n",
    "print('Recall: %.3f (%.3f)' % (mean(recall_results_ada), std(recall_results_ada)))\n",
    "print('AUC_ROC: %.3f (%.3f)' % (mean(aucroc_results_ada), std(aucroc_results_ada)))\n",
    "print('MCC: %.3f (%.3f)' % (mean(mcc_results_ada), std(mcc_results_ada)))\n",
    "\n",
    "# Prepare the DataFrame for results\n",
    "data_ada_s = pd.DataFrame(data=[['AdaBoost',\n",
    "                                 '%.3f (%.3f)' % (mean(acc_results_ada), std(acc_results_ada)),\n",
    "                                 '%.3f (%.3f)' % (mean(aucroc_results_ada), std(aucroc_results_ada)),\n",
    "                                 '%.3f (%.3f)' % (mean(F1_results_ada), std(F1_results_ada)),\n",
    "                                 '%.3f (%.3f)' % (mean(mcc_results_ada), std(mcc_results_ada))]],\n",
    "                          columns=['Classifier', 'Accuracy', 'AUC-ROC', 'F1 score', 'MCC'])\n",
    "data_classifier_s = pd.concat([data_classifier_s, data_ada_s], ignore_index=True)\n",
    "\n",
    "# Plot the mean ROC curve across all folds\n",
    "mean_tpr_ada_s = np.mean(tprs, axis=0)\n",
    "mean_auc_ada_s = auc(mean_fpr_ada_s, mean_tpr_ada_s)\n",
    "std_auc_ada = np.std(aucs_ada)\n",
    "\n",
    "plt.plot(mean_fpr_ada_s, mean_tpr_ada_s, color='b', label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc_ada_s, std_auc_ada), lw=2, alpha=.8)\n",
    "\n",
    "# Plot the chance line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "\n",
    "# Finalize the plot\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Nested Cross-Validation ROC Curve for AdaBoost Classifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_ada_s', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize lists to store results\n",
    "acc_results_s = []\n",
    "precision_results_s = []\n",
    "F1_results_s = []\n",
    "recall_results_s = []\n",
    "aucroc_results_s = []\n",
    "mcc_results_s = []\n",
    "aucs_s = []\n",
    "\n",
    "# Store probability predictions and true labels here\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "y_prob_all_SVM = []\n",
    "\n",
    "selected_features_list = []\n",
    "best_params_list = []\n",
    "cross_val_scores_list = []\n",
    "\n",
    "# Calibration\n",
    "svm_kfold_probability = []\n",
    "kfold_true_label_svm = []\n",
    "\n",
    "fold = 0\n",
    "\n",
    "mean_fpr_s_s = np.linspace(0, 1, 100)\n",
    "\n",
    "for train_ix, test_ix in cv_outer.split(X_s):\n",
    "    # Split data\n",
    "    X_train, X_test = X_s[train_ix], X_s[test_ix]\n",
    "    y_train, y_test = y_s[train_ix], y_s[test_ix]\n",
    "    fold += 1\n",
    "    \n",
    "    # Configure the cross-validation procedure\n",
    "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    \n",
    "    # Feature selection\n",
    "    model_rfc = rfc.fit(X_train, y_train)\n",
    "    feature_importances = model_rfc.feature_importances_\n",
    "    selected_features = np.where(feature_importances > 0.01)[0]  # Set a threshold for feature selection\n",
    "    selected_features_list.append(selected_features)\n",
    "    \n",
    "    # Subset the features in the training and test sets\n",
    "    X_train_fs = X_train[:, selected_features]\n",
    "    X_test_fs = X_test[:, selected_features]\n",
    "    \n",
    "    # Define the model with default hyperparameters\n",
    "    model = SVC(kernel='rbf', class_weight='balanced', probability=True)\n",
    "    \n",
    "    # Define the grid of values to search\n",
    "    param_grid = {'C': [1e-10, 1e-5, 1e-3, 1e-2, 0.1, 1, 1e1, 1e2, 1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "                  'gamma': [1e-6, 1e-5, 1e-4, 0.001, 0.005, 0.01, 0.1, 1, 10, 100]}\n",
    "    \n",
    "    # Define search\n",
    "    search_s = GridSearchCV(model, param_grid, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "    \n",
    "    # Execute the grid search\n",
    "    result_s = search_s.fit(X_train_fs, y_train)\n",
    "    best_model_s = result_s.best_estimator_\n",
    "    \n",
    "    # Evaluate model on the hold-out dataset\n",
    "    probas_svc = best_model_s.predict_proba(X_test_fs)\n",
    "    y_pred_s = best_model_s.predict_proba(X_test_fs)[:, 1]\n",
    "    \n",
    "    # Compute and plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_s)\n",
    "    tprs.append(np.interp(mean_fpr_s_s, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs_s.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1.5, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (fold, roc_auc))\n",
    "    \n",
    "    yhat_s = best_model_s.predict(X_test_fs)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    acc_s = accuracy_score(y_test, yhat_s)\n",
    "    precision_s = precision_score(y_test, yhat_s)\n",
    "    recall_s = recall_score(y_test, yhat_s)\n",
    "    f1_s = f1_score(y_test, yhat_s)\n",
    "    mcc_s = matthews_corrcoef(y_test, yhat_s)\n",
    "    auc_s = metrics.roc_auc_score(y_test, probas_svc[:, 1])\n",
    "    \n",
    "    # Calibration\n",
    "    svm_kfold_probability.append(probas_svc[:, 1])\n",
    "    kfold_true_label_svm.append(y_test)\n",
    "    \n",
    "    # Store the result\n",
    "    acc_results_s.append(acc_s)\n",
    "    precision_results_s.append(precision_s)\n",
    "    F1_results_s.append(f1_s)\n",
    "    recall_results_s.append(recall_s)\n",
    "    aucroc_results_s.append(auc_s)\n",
    "    mcc_results_s.append(mcc_s)\n",
    "    \n",
    "    # Report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc_s, result_s.best_score_, result_s.best_params_))\n",
    "\n",
    "# Plot the mean ROC curve across all folds\n",
    "mean_tpr_s_s = np.mean(tprs, axis=0)\n",
    "mean_auc_s_s = auc(mean_fpr_s_s, mean_tpr_s_s)\n",
    "std_auc_s_s = np.std(aucs_s)\n",
    "\n",
    "# Summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(acc_results_s), std(acc_results_s)))\n",
    "print('Precision: %.3f (%.3f)' % (mean(precision_results_s), std(precision_results_s)))\n",
    "print('F1_Score: %.3f (%.3f)' % (mean(F1_results_s), std(F1_results_s)))\n",
    "print('Recall: %.3f (%.3f)' % (mean(recall_results_s), std(recall_results_s)))\n",
    "print('AUC_ROC: %.3f (%.3f)' % (mean_auc_s_s, std(aucroc_results_s)))\n",
    "print('MCC: %.3f (%.3f)' % (mean(mcc_results_s), std(mcc_results_s)))\n",
    "\n",
    "# Prepare the DataFrame for results\n",
    "data_svm_s = pd.DataFrame(data=[['SVM',\n",
    "                                 '%.3f (%.3f)' % (mean(acc_results_s), std(acc_results_s)),\n",
    "                                 '%.3f (%.3f)' % (mean(aucroc_results_s), std(aucroc_results_s)),\n",
    "                                 '%.3f (%.3f)' % (mean(F1_results_s), std(F1_results_s)),\n",
    "                                 '%.3f (%.3f)' % (mean(mcc_results_s), std(mcc_results_s))]],\n",
    "                          columns=['Classifier', 'Accuracy', 'AUC-ROC', 'F1 score', 'MCC'])\n",
    "data_classifier_s = pd.concat([data_classifier_s, data_svm_s], ignore_index=True)\n",
    "\n",
    "# Plot the mean ROC curve across all folds\n",
    "mean_tpr_s_s = np.mean(tprs, axis=0)\n",
    "mean_auc_s_s = auc(mean_fpr_s_s, mean_tpr_s_s)\n",
    "std_auc_s_s = np.std(aucs_s)\n",
    "\n",
    "plt.plot(mean_fpr_s_s, mean_tpr_s_s, color='b', label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc_s_s, std_auc_s_s), lw=2, alpha=.8)\n",
    "\n",
    "# Plot the chance line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "\n",
    "# Finalize the plot\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Nested Cross-Validation ROC Curve for SVM Classifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_svm_s', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    "\n",
    "# Configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize lists to store results\n",
    "acc_results_r = []\n",
    "precision_results_r = []\n",
    "F1_results_r = []\n",
    "recall_results_r = []\n",
    "mcc_results_r = []  # List to store MCC results\n",
    "aucroc_results_r = []\n",
    "aucs_r = []\n",
    "r_kfold_probability = []\n",
    "kfold_true_label_r = []\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "fold = 0\n",
    "mean_fpr_r_s = np.linspace(0, 1, 100)\n",
    "\n",
    "# Main cross-validation loop\n",
    "for train_ix, test_ix in cv_outer.split(X_s):\n",
    "    fold += 1\n",
    "    X_train, X_test = X_s[train_ix, :], X_s[test_ix, :]\n",
    "    y_train, y_test = y_s[train_ix], y_s[test_ix]\n",
    "\n",
    "    # Feature selection\n",
    "    rfc = RandomForestClassifier(random_state=1)\n",
    "    model_rfc = rfc.fit(X_train, y_train)\n",
    "    feature_importances = rfc.feature_importances_\n",
    "    selected_features = np.where(feature_importances > 0.01)[0]\n",
    "    \n",
    "    # Subset the features in the training and test sets\n",
    "    X_train_fs = X_train[:, selected_features]\n",
    "    X_test_fs = X_test[:, selected_features]\n",
    "    \n",
    "    # Define the model with default hyperparameters\n",
    "    model = RandomForestClassifier(random_state=22)\n",
    "    param_grid = {\n",
    "        'n_estimators': [1, 10, 100],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [1, 2, 3, 4],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "    \n",
    "    # Grid search\n",
    "    search = GridSearchCV(model, param_grid, scoring='accuracy', cv=KFold(n_splits=3, shuffle=True, random_state=1), refit=True)\n",
    "    result = search.fit(X_train_fs, y_train)\n",
    "    best_model = result.best_estimator_\n",
    "    \n",
    "    # Evaluate model\n",
    "    probas_r = best_model.predict_proba(X_test_fs)\n",
    "    yhat_r = best_model.predict(X_test_fs)\n",
    "    y_pred_r = probas_r[:, 1]\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_r)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs_r.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1.5, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (fold, roc_auc))\n",
    "\n",
    "    acc_r = accuracy_score(y_test, yhat_r)\n",
    "    precision_r = precision_score(y_test, yhat_r)\n",
    "    recall_r = recall_score(y_test, yhat_r)\n",
    "    f1_r = f1_score(y_test, yhat_r)\n",
    "    mcc_r = matthews_corrcoef(y_test, yhat_r)  # Calculate MCC\n",
    "    auc_r = roc_auc_score(y_test, y_pred_r)\n",
    "\n",
    "    # Store the result\n",
    "    acc_results_r.append(acc_r)\n",
    "    precision_results_r.append(precision_r)\n",
    "    F1_results_r.append(f1_r)\n",
    "    recall_results_r.append(recall_r)\n",
    "    mcc_results_r.append(mcc_r)  # Store MCC result\n",
    "    aucroc_results_r.append(auc_r)\n",
    "\n",
    "    r_kfold_probability.append(probas_r[:, 1])\n",
    "    kfold_true_label_r.append(y_test)\n",
    "\n",
    "    # Report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s, mcc=%.3f' % (acc_r, result.best_score_, result.best_params_, mcc_r))\n",
    "    \n",
    "# Summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(acc_results_r), np.std(acc_results_r)))\n",
    "print('Precision: %.3f (%.3f)' % (np.mean(precision_results_r), np.std(precision_results_r)))\n",
    "print('F1_Score: %.3f (%.3f)' % (np.mean(F1_results_r), np.std(F1_results_r)))\n",
    "print('Recall: %.3f (%.3f)' % (np.mean(recall_results_r), np.std(recall_results_r)))\n",
    "print('MCC: %.3f (%.3f)' % (np.mean(mcc_results_r), np.std(mcc_results_r)))  # Print MCC result\n",
    "print('AUC_ROC: %.3f (%.3f)' % (np.mean(aucroc_results_r), np.std(aucroc_results_r)))\n",
    "\n",
    "# Plot the mean ROC curve across all folds\n",
    "mean_tpr_r_s = np.mean(tprs, axis=0)\n",
    "mean_auc_r_s = auc(mean_fpr_r_s, mean_tpr_r_s)\n",
    "std_auc_r = np.std(aucs_r)\n",
    "plt.plot(mean_fpr_r_s, mean_tpr_r_s, color='b', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc_r_s, std_auc_r), lw=2, alpha=.8)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "\n",
    "# Finalize plot settings\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Nested Cross-Validation ROC Curve for Random Forest Classifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_Randomforest', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "\n",
    "\n",
    "\n",
    "# Configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize lists to store results\n",
    "acc_results_x = []\n",
    "precision_results_x = []\n",
    "F1_results_x = []\n",
    "recall_results_x = []\n",
    "aucroc_results_x = []\n",
    "mcc_results_x = []\n",
    "aucs_x = []\n",
    "tprs = []\n",
    "selected_features_list = []\n",
    "\n",
    "# Store probability predictions and true labels here\n",
    "x_kfold_probability = []\n",
    "kfold_true_label_x = []\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "fold = 0\n",
    "iter_shap = 0\n",
    "indices = []\n",
    "df_avg_train = []\n",
    "df_avg_test = []\n",
    "\n",
    "mean_fpr_x = np.linspace(0, 1, 100)\n",
    "\n",
    "# Optimal threshold\n",
    "optimal_threshold = 0.5  # You might want to optimize this further\n",
    "\n",
    "for train_ix, test_ix in cv_outer.split(X_s):\n",
    "     # split data\n",
    "    X_train, X_test = X_s[train_ix, :], X_s[test_ix, :]\n",
    "    y_train, y_test = y_s[train_ix], y_s[test_ix]\n",
    "    # configure the cross-validation procedure\n",
    "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    fold += 1\n",
    "    \n",
    "    # Feature selection\n",
    "    model_rfc = RandomForestClassifier(random_state=42).fit(X_train, y_train)\n",
    "    feature_importances = model_rfc.feature_importances_\n",
    "    selected_features = np.where(feature_importances > 0.01)[0]  # Set a threshold for feature selection\n",
    "    selected_features_list.append(selected_features)\n",
    "   \n",
    "    # Subset the features in the training and test sets\n",
    "    X_train_fs = X_train[:, selected_features]\n",
    "    X_test_fs = X_test[:, selected_features]\n",
    "    \n",
    "    # Define the model with default hyperparameters\n",
    "    model = XGBClassifier(n_jobs=-1)\n",
    "    \n",
    "    # Define the grid of values to search\n",
    "    param_grid_d = {\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "        'max_depth': [3, 5, 7, 9, 11],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'reg_alpha': [0, 0.001, 0.01, 0.1, 0.5],\n",
    "        'reg_lambda': [0, 0.001, 0.01, 0.1, 0.5],\n",
    "        'n_estimators': [10, 100, 200, 500]\n",
    "    }\n",
    "    param_grid = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0,0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "    }\n",
    "    \n",
    "    # Define search\n",
    "    search = GridSearchCV(model, param_grid, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "    \n",
    "    # Execute the grid search\n",
    "    result = search.fit(X_train_fs, y_train)\n",
    "    best_model = result.best_estimator_\n",
    "    \n",
    "    y_pred_x = best_model.predict_proba(X_test_fs)[:, 1]\n",
    "    \n",
    "    # Compute and plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_x)\n",
    "    tprs.append(np.interp(mean_fpr_x, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs_x.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1.5, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (fold, roc_auc))\n",
    "    \n",
    "    # Evaluate model on the hold out dataset\n",
    "    probas_x = best_model.predict_proba(X_test_fs)\n",
    "\n",
    "    yhat_x = best_model.predict(X_test_fs)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    acc_x = accuracy_score(y_test, yhat_x)\n",
    "    precision_x = precision_score(y_test, yhat_x)\n",
    "    recall_x = recall_score(y_test, yhat_x)\n",
    "    f1_x = f1_score(y_test, yhat_x)\n",
    "    \n",
    "    # Optimize MCC with the threshold\n",
    "    yhat_x_optimized = (probas_x[:, 1] >= optimal_threshold).astype(int)\n",
    "    mcc_x = matthews_corrcoef(y_test, yhat_x_optimized)\n",
    "    \n",
    "    # Evaluate AUC\n",
    "    auc_x = metrics.roc_auc_score(y_test, probas_x[:, 1])\n",
    "    \n",
    "    ## Calibration\n",
    "    x_kfold_probability.append(probas_x[:, 1])\n",
    "    kfold_true_label_x.append(y_test)\n",
    "    \n",
    "    # Store the result\n",
    "    acc_results_x.append(acc_x)\n",
    "    precision_results_x.append(precision_x)\n",
    "    recall_results_x.append(recall_x)\n",
    "    F1_results_x.append(f1_x)\n",
    "    aucroc_results_x.append(auc_x)\n",
    "    mcc_results_x.append(mcc_x)\n",
    "    \n",
    "    # Report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc_x, result.best_score_, result.best_params_))\n",
    "    '''\n",
    "    # Defining the SHAP value classifier\n",
    "    classifier_shap = best_model\n",
    "    classifier_shap.fit(X_train, y_train)\n",
    "\n",
    "    # SHAP values for training data\n",
    "    train_tmp_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "    explainer = shap.TreeExplainer(classifier_shap, train_tmp_df, feature_perturbation='interventional', model_output='probability')\n",
    "\n",
    "    # SHAP values for test data\n",
    "    test_tmp_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "    test_explainer = shap.TreeExplainer(classifier_shap, test_tmp_df, feature_perturbation='interventional', model_output='probability')\n",
    "\n",
    "    df_shap_train = pd.DataFrame(explainer.shap_values(train_tmp_df), columns=X.columns)\n",
    "    df_shap_train[\"index\"] = train_ix\n",
    "    df_avg_train.append(df_shap_train)\n",
    "\n",
    "    test_df_shap = pd.DataFrame(test_explainer.shap_values(test_tmp_df), columns=X.columns)\n",
    "    test_df_shap[\"index\"] = test_ix\n",
    "    df_avg_test.append(test_df_shap)\n",
    "\n",
    "    iter_shap += 1\n",
    "'''\n",
    "\n",
    "# Summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(acc_results_x), np.std(acc_results_x)))\n",
    "print('Precision: %.3f (%.3f)' % (np.mean(precision_results_x), np.std(precision_results_x)))\n",
    "print('F1_Score: %.3f (%.3f)' % (np.mean(F1_results_x), np.std(F1_results_x)))\n",
    "print('Recall: %.3f (%.3f)' % (np.mean(recall_results_x), np.std(recall_results_x)))\n",
    "print('AUC_ROC: %.3f (%.3f)' % (np.mean(aucroc_results_x), np.std(aucroc_results_x)))\n",
    "print('MCC: %.3f (%.3f)' % (np.mean(mcc_results_x), np.std(mcc_results_x)))\n",
    "\n",
    "# Results\n",
    "all_scores_x = {\n",
    "    \"Accuracy\": acc_results_x,\n",
    "    'AUC-ROC': aucroc_results_x,\n",
    "    'F1-Score': F1_results_x,\n",
    "    'Recall': recall_results_x,\n",
    "    'Precision': precision_results_x,\n",
    "    'MCC': mcc_results_x,\n",
    "    'Classifier': 'XGBoost'\n",
    "}\n",
    "\n",
    "all_scores_x = pd.DataFrame(all_scores_x)\n",
    "\n",
    "# Plot the mean ROC curve across all folds\n",
    "mean_tpr_x = np.mean(tprs, axis=0)\n",
    "mean_auc_x = auc(mean_fpr_x, mean_tpr_x)\n",
    "std_auc_x = np.std(aucs_x)\n",
    "\n",
    "plt.plot(mean_fpr_x, mean_tpr_x, color='b', label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc_x, std_auc_x), lw=2, alpha=.8)\n",
    "\n",
    "# Plot the chance line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "\n",
    "# Finalize the plot\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Nested Cross-Validation ROC Curve for XGBoost Classifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_xgboost_s', dpi=300)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Append new data to data_classifier\n",
    "data_xgboost_s = pd.DataFrame(data=[['XGBoost',\n",
    "                                   '%.3f (%.3f)' % (np.mean(acc_results_x), np.std(acc_results_x)),\n",
    "                                   '%.3f (%.3f)' % (np.mean(aucroc_results_x), np.std(aucroc_results_x)),\n",
    "                                   '%.3f (%.3f)' % (np.mean(F1_results_x), np.std(F1_results_x)),\n",
    "                                   '%.3f (%.3f)' % (np.mean(mcc_results_x), np.std(mcc_results_x))]],\n",
    "                            columns=['Classifier', 'Accuracy', 'AUC-ROC', 'F1 score', 'MCC'])\n",
    "\n",
    "data_classifier_s = pd.concat([data_classifier_s, data_xgboost_s], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Configuration for cross-validation\n",
    "cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize results storage\n",
    "acc_results_k = []\n",
    "precision_results_k = []\n",
    "F1_results_k = []\n",
    "recall_results_k = []\n",
    "aucroc_results_k = []\n",
    "mcc_results_k = []\n",
    "\n",
    "# ROC Curve preparation\n",
    "tprs = []\n",
    "aucs_k = []\n",
    "mean_fpr_k = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "fold = 0\n",
    "\n",
    "# Main cross-validation loop\n",
    "for train_ix, test_ix in cv_outer.split(X_s):\n",
    "    fold += 1\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X_s[train_ix, :], X_s[test_ix, :]\n",
    "    y_train, y_test = y_s[train_ix], y_s[test_ix]\n",
    "    \n",
    "    # Feature selection using RandomForestClassifier\n",
    "    rfc = RandomForestClassifier(random_state=1)\n",
    "    model_rfc = rfc.fit(X_train, y_train)\n",
    "    feature_importances = rfc.feature_importances_\n",
    "    selected_features = np.where(feature_importances > 0.01)[0]\n",
    "    \n",
    "    # Subset the features in the training and test sets\n",
    "    X_train_fs, X_test_fs = X_train[:, selected_features], X_test[:, selected_features]\n",
    "    \n",
    "    # Define and configure the KNN model\n",
    "    model = KNeighborsClassifier()\n",
    "    param_grid = {\n",
    "        'n_neighbors': [1, 3, 5, 7, 6, 8, 10, 12],\n",
    "        'leaf_size': list(range(1, 50))\n",
    "    }\n",
    "    \n",
    "    # Grid search within the inner cross-validation loop\n",
    "    search_k = GridSearchCV(model, param_grid, scoring='accuracy', cv=KFold(n_splits=3, shuffle=True, random_state=1), refit=True)\n",
    "    result = search_k.fit(X_train_fs, y_train)\n",
    "    best_model = result.best_estimator_\n",
    "    \n",
    "    # Predict probabilities and calculate ROC curve\n",
    "    y_pred_k = best_model.predict_proba(X_test_fs)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_k)\n",
    "    tprs.append(np.interp(mean_fpr_k, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs_k.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1.5, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (fold, roc_auc))\n",
    "\n",
    "    # Predictions for evaluation\n",
    "    yhat_k = best_model.predict(X_test_fs)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    acc_k = accuracy_score(y_test, yhat_k)\n",
    "    precision_k = precision_score(y_test, yhat_k)\n",
    "    recall_k = recall_score(y_test, yhat_k)\n",
    "    f1_k = f1_score(y_test, yhat_k)\n",
    "    mcc_k = matthews_corrcoef(y_test, yhat_k)\n",
    "    \n",
    "    # Store results\n",
    "    acc_results_k.append(acc_k)\n",
    "    precision_results_k.append(precision_k)\n",
    "    F1_results_k.append(f1_k)\n",
    "    recall_results_k.append(recall_k)\n",
    "    aucroc_results_k.append(roc_auc)\n",
    "    mcc_results_k.append(mcc_k)\n",
    "    \n",
    "    # Report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s, mcc=%.3f' % (acc_k, result.best_score_, result.best_params_, mcc_k))\n",
    "\n",
    "# Summarize estimated performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(acc_results_k), std(acc_results_k)))\n",
    "print('Precision: %.3f (%.3f)' % (mean(precision_results_k), std(precision_results_k)))\n",
    "print('F1_Score: %.3f (%.3f)' % (mean(F1_results_k), std(F1_results_k)))\n",
    "print('Recall: %.3f (%.3f)' % (mean(recall_results_k), std(recall_results_k)))\n",
    "print('AUC_ROC: %.3f (%.3f)' % (mean(aucroc_results_k), std(aucroc_results_k)))\n",
    "print('MCC: %.3f (%.3f)' % (mean(mcc_results_k), std(mcc_results_k)))\n",
    "\n",
    "# Plot the mean ROC curve across all folds\n",
    "mean_tpr_k = np.mean(tprs, axis=0)\n",
    "mean_auc_k = auc(mean_fpr_k, mean_tpr_k)\n",
    "std_auc_k = np.std(aucs_k)\n",
    "plt.plot(mean_fpr_k, mean_tpr_k, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc_k, std_auc_k), lw=2, alpha=.8)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Nested Cross-Validation ROC Curve for KNN Classifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_knn', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
